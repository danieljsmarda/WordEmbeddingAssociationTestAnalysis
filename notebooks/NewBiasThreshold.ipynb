{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import random\n",
    "random.seed(5)\n",
    "import sys\n",
    "import random\n",
    "sys.path.append('../src')\n",
    "from models import word_set_to_mtx, save_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\Miniconda3\\envs\\semproject\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 400000\n"
     ]
    }
   ],
   "source": [
    "we_model_name = \"sg_dim300_min100_win5\"\n",
    "we_vector_size = 300\n",
    "we_model_dir = '../data/external/wiki-english/wiki-english-20171001/%s' % we_model_name\n",
    "\n",
    "we_model = Word2Vec.load(we_model_dir+'/model.gensim')\n",
    "print ('loading done!')\n",
    "print(f'Total words: {len(we_model.wv.vocab)}')\n",
    "\n",
    "\n",
    "'''\n",
    "# Caliskan GloVe\n",
    "glove_file = '../data/external/glove.6B/glove.6B.50d.txt'\n",
    "_ = glove2word2vec(glove_file, '../data/interim/tmp.txt')\n",
    "we_model = KeyedVectors.load_word2vec_format('../data/interim/tmp.txt')\n",
    "print('loading done!')\n",
    "print(f'Total words: {len(we_model.wv.vocab)}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_BIASES_PATH_2NDORDER = '../data/processed/threshold_biases_2ndorder.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wftv', 'stollwerck', 'dansville']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = list(we_model.wv.vocab.keys())\n",
    "random.choices(vocabulary, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_1 = random.choices(vocabulary, k=3)\n",
    "set_2 = random.choices(vocabulary, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1] [1 1 1]\n",
      "[ 2  5 10] [5 5 5]\n",
      "[ 3  5 10] [10 10 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_1 = np.array([[1, 1, 1], [2,5,10], [3,5,10]])\n",
    "set_2 = np.array([[1, 1, 1], [5, 5, 5], [10,10,10]])\n",
    "zip(set_1, set_2)\n",
    "[print(u,v) for u,v in zip(set_1, set_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.86415857, 0.8977584 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def matrix_cosine(x, y):\n",
    "    '''If x is NxM array and y is NxM array, this function\n",
    "    calculates row-wise cosine similary, so the results is a\n",
    "    array with shape (N,).'''\n",
    "    return np.einsum('ij,ij->i', x, y) / (\n",
    "              np.linalg.norm(x, axis=1) * np.linalg.norm(y, axis=1)\n",
    "    )\n",
    "print(type(set_1))\n",
    "print(matrix_cosine(set_1, set_2).shape)\n",
    "matrix_cosine(set_1, set_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14972538, 0.03376701, 0.23382741, ..., 0.098508  , 0.08962049,\n",
       "       0.02198978], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_biases(we_model, k_pairs=500000, order='second'):\n",
    "    #TODO: refactor: remove 'order' argument, no longer necessary\n",
    "    vocabulary = list(we_model.wv.vocab.keys())\n",
    "    set_1 = random.choices(vocabulary, k=k_pairs)\n",
    "    set_2 = random.choices(vocabulary, k=k_pairs)\n",
    "    arr_1 = word_set_to_mtx(we_model, set_1)\n",
    "    arr_2 = word_set_to_mtx(we_model, set_2)\n",
    "    biases = matrix_cosine(arr_1, arr_2)\n",
    "    save_pickle(biases, THRESHOLD_BIASES_PATH_2NDORDER)\n",
    "    return biases\n",
    "get_biases(we_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
