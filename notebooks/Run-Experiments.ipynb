{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import get_2ndorder_association_metric_list_for_target_list, \\\n",
    "    get_matrices_from_term_lists, \\\n",
    "    save_arrays, open_pickle, save_pickle, \\\n",
    "    save_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading done!\n",
      "Total words: 2196016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\Miniconda3\\envs\\semproject2\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Glove model fast load\n",
    "we_model = KeyedVectors.load('../data/interim/glove_840B_normed', mmap='r')\n",
    "print('loading done!')\n",
    "print(f'Total words: {len(we_model.wv.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_DEFINITION_PATH = '../data/interim/glove_840B_experiment_definitions.pickle'\n",
    "RESULTS_FILEPATH = '../data/interim/glove_840B_association_metric_exps.pickle'\n",
    "SCALERS_FILEPATH = '../data/processed/glove_840B_scalers.pickle'\n",
    "THRESHOLD_BIASES_PATH_2NDORDER = '../data/processed/glove_840B_threshold_biases_2ndorder.pickle'\n",
    "THRESHOLD_BIASES_PATH_1STORDER = '../data/processed/glove_840B_threshold_biases_1storder.pickle'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosines_for_target_word_unscaled(word_vec, A_mtx, B_mtx):\n",
    "    A_dot_v = np.dot(A_mtx, word_vec)\n",
    "    B_dot_v = np.dot(B_mtx, word_vec)\n",
    "    A_norms = np.multiply(np.linalg.norm(A_mtx, axis=1), np.linalg.norm(word_vec))\n",
    "    B_norms = np.multiply(np.linalg.norm(B_mtx, axis=1), np.linalg.norm(word_vec))\n",
    "    A_cosines = np.divide(A_dot_v, A_norms)\n",
    "    B_cosines = np.divide(B_dot_v, B_norms)\n",
    "    return np.mean(A_cosines), np.mean(B_cosines)\n",
    "\n",
    "def calculate_cosines_for_all_words_unscaled(we_model, A_mtx, B_mtx):\n",
    "    '''Computes the association metric, s(w,A,B).\n",
    "    A_mtx, B_mtx: 2-D word vector arrays'''\n",
    "    # We also tried an alternative implementation using the following lines\n",
    "    # A_cosines_apply = np.apply_along_axis(lambda row: 1-cosine_distance(row, word_vec), 1, A_mtx)\n",
    "    # B_cosines_apply = np.apply_along_axis(lambda row: 1-cosine_distance(row, word_vec), 1, B_mtx)\n",
    "    # but we found that the norm-based implementation was faster.\n",
    "    A_mtx_norm = A_mtx/np.linalg.norm(A_mtx, axis=1).reshape(-1,1)\n",
    "    B_mtx_norm = B_mtx/np.linalg.norm(B_mtx, axis=1).reshape(-1,1)\n",
    "    all_mtx_norm = we_model.wv.vectors/np.linalg.norm(we_model.wv.vectors, axis=1).reshape(-1,1)\n",
    "    \n",
    "    all_associations_to_A = np.dot(A_mtx_norm, all_mtx_norm.T)\n",
    "    all_associations_to_B = np.dot(B_mtx_norm, all_mtx_norm.T)\n",
    "    \n",
    "    return np.mean(all_associations_to_A, axis=0), np.mean(all_associations_to_B, axis=0)\n",
    "\n",
    "def add_quantile_ranges_to_dict(dct, biases):\n",
    "    dct['QR_95'] = [np.percentile(biases, 2.5), np.percentile(biases, 97.5)]\n",
    "    dct['QR_99'] = [np.percentile(biases, 0.5), np.percentile(biases, 99.5)]\n",
    "    dct['QR_99.9'] = [np.percentile(biases, 0.05), np.percentile(biases, 99.95)]\n",
    "\n",
    "\n",
    "def get_2ndorder_association_metric_list_for_target_list(target_list, A_terms, B_terms, we_model, exp_num):\n",
    "    \n",
    "    [X_mtx, _, A_mtx, B_mtx] = get_matrices_from_term_lists(we_model, target_list, target_list, A_terms, B_terms)\n",
    "    \n",
    "    # A_associations, B_associations are associations for all words    \n",
    "    A_associations, B_associations = calculate_cosines_for_all_words_unscaled(we_model, A_mtx, B_mtx)\n",
    "    \n",
    "    \n",
    "    all_associations = np.concatenate((A_associations, B_associations))\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler.fit(all_associations.reshape(-1,1))\n",
    "    save_scalers(SCALERS_FILEPATH, exp_num, 'second', scaler)\n",
    "    \n",
    "    _th = np.mean(np.abs(A_associations - B_associations))\n",
    "    _th = scaler.transform(_th.reshape(-1, 1))[0,0]\n",
    "    \n",
    "    biases = A_associations - B_associations\n",
    "    biases = scaler.transform(biases.reshape(-1, 1))\n",
    "    QR_dict = add_quantile_ranges_to_dict({}, biases)                                                             \n",
    "\n",
    "    target_associations = np.apply_along_axis(lambda x_vec: calculate_cosines_for_target_word_unscaled(x_vec, A_mtx, B_mtx), 1, X_mtx)\n",
    "    \n",
    "    target_biases = []\n",
    "    A_biases = []\n",
    "    for _assoc in target_associations:\n",
    "        _A_assoc = scaler.transform(_assoc[0].reshape(-1, 1))[0,0]\n",
    "        _B_assoc = scaler.transform(_assoc[1].reshape(-1, 1))[0,0]\n",
    "        _bias = _A_assoc - _B_assoc\n",
    "        target_biases.append(_bias)\n",
    "        A_biases.append(_A_assoc)\n",
    "    return np.array(target_biases), _th, np.array(A_biases), QR_dict\n",
    "\n",
    "def run_exps_2ndorder(X_terms, Y_terms, A_terms, B_terms, exp_num):\n",
    "    order='second'\n",
    "    X_metrics, _th, A_biases, QR_dict = get_2ndorder_association_metric_list_for_target_list(X_terms, A_terms, B_terms, we_model, exp_num)\n",
    "    Y_metrics, _th, A_biases, QR_dict = get_2ndorder_association_metric_list_for_target_list(Y_terms, A_terms, B_terms, we_model, exp_num)\n",
    "    print (X_metrics)\n",
    "    print (Y_metrics)\n",
    "\n",
    "    print ('mean bias to X', np.mean(X_metrics))\n",
    "    print ('mean bias to Y', np.mean(Y_metrics))\n",
    "\n",
    "    print ('Bias threshold', _th)\n",
    "\n",
    "    order = 'second'\n",
    "    threshold = _th\n",
    "    save_arrays(RESULTS_FILEPATH, exp_num, order, X_metrics, Y_metrics, threshold, A_biases, QR_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'open_pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-403dc4e59a9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mB_terms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'B_terms'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mrun_exps_2ndorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mrun_all_exps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-403dc4e59a9c>\u001b[0m in \u001b[0;36mrun_all_exps\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun_all_exps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mexps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEXPERIMENT_DEFINITION_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mexp_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'***********************************'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Experiment: {exp_num}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'open_pickle' is not defined"
     ]
    }
   ],
   "source": [
    "def run_all_exps():\n",
    "    exps = open_pickle(EXPERIMENT_DEFINITION_PATH)\n",
    "    for exp_num, exp in exps.items():\n",
    "        print('***********************************')\n",
    "        print(f'Experiment: {exp_num}')\n",
    "        X_terms = exp['X_terms']\n",
    "        Y_terms = exp['Y_terms']\n",
    "        A_terms = exp['A_terms']\n",
    "        B_terms = exp['B_terms']\n",
    "        run_exps_2ndorder(X_terms, Y_terms, A_terms, B_terms, exp_num)\n",
    "run_all_exps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
